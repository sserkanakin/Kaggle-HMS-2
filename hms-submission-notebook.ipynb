{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1950 processed patient files; skipping minimal preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Quick minimal preprocessing if no processed patients exist\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from src.data.utils.eeg_process import EEGGraphBuilder, select_eeg_channels\n",
    "from src.data.utils.spectrogram_process import SpectrogramGraphBuilder, filter_spectrogram_columns\n",
    "\n",
    "proc_dir = Path('data/processed')\n",
    "proc_dir.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(proc_dir.glob('patient_*.pt'))\n",
    "if len(existing) == 0:\n",
    "    print('No processed patients found; generating a tiny sample for smoke training...')\n",
    "    cfg = OmegaConf.load('configs/graphs.yaml')\n",
    "    df = pd.read_csv(cfg.paths.train_csv)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    # Pick first patient with at least 1-2 samples\n",
    "    first_pid = int(df['patient_id'].iloc[0])\n",
    "    df_pid = df[df['patient_id'] == first_pid].head(2).reset_index(drop=True)\n",
    "    print('Using patient', first_pid, 'with', len(df_pid), 'samples')\n",
    "\n",
    "    # Build graph builders\n",
    "    eeg_builder = EEGGraphBuilder(\n",
    "        sampling_rate=cfg.eeg.sampling_rate,\n",
    "        window_size=cfg.eeg.window_size,\n",
    "        stride=cfg.eeg.stride,\n",
    "        bands=dict(cfg.eeg.bands),\n",
    "        coherence_threshold=cfg.eeg.coherence.threshold,\n",
    "        nperseg_factor=cfg.eeg.coherence.nperseg_factor,\n",
    "        channels=list(cfg.eeg.channels),\n",
    "        apply_bandpass=cfg.eeg.preprocessing.bandpass_filter.enabled,\n",
    "        bandpass_low=cfg.eeg.preprocessing.bandpass_filter.lowcut,\n",
    "        bandpass_high=cfg.eeg.preprocessing.bandpass_filter.highcut,\n",
    "        bandpass_order=cfg.eeg.preprocessing.bandpass_filter.order,\n",
    "        apply_notch=cfg.eeg.preprocessing.notch_filter.enabled,\n",
    "        notch_freq=cfg.eeg.preprocessing.notch_filter.frequency,\n",
    "        notch_q=cfg.eeg.preprocessing.notch_filter.quality_factor,\n",
    "        apply_normalize=cfg.eeg.preprocessing.normalize.enabled,\n",
    "    )\n",
    "    spec_builder = SpectrogramGraphBuilder(\n",
    "        window_size=cfg.spectrogram.window_size,\n",
    "        stride=cfg.spectrogram.stride,\n",
    "        regions=list(cfg.spectrogram.regions),\n",
    "        bands=dict(cfg.spectrogram.bands),\n",
    "        aggregation=cfg.spectrogram.aggregation,\n",
    "        spatial_edges=cfg.spectrogram.spatial_edges,\n",
    "        apply_preprocessing=cfg.spectrogram.preprocessing.enabled,\n",
    "        clip_min=cfg.spectrogram.preprocessing.clip_min,\n",
    "        clip_max=cfg.spectrogram.preprocessing.clip_max,\n",
    "    )\n",
    "\n",
    "    # Produce graphs for up to 2 labels and save in one patient file\n",
    "    patient_data = {}\n",
    "    for _, row in df_pid.iterrows():\n",
    "        eeg_path = f\"{cfg.paths.train_eegs}/{row.eeg_id}.parquet\"\n",
    "        eeg_df = pd.read_parquet(eeg_path)\n",
    "        eeg_offset = int(row.eeg_label_offset_seconds)\n",
    "        eeg_start = eeg_offset * 200\n",
    "        eeg_end = eeg_start + 50 * 200\n",
    "        eeg_window_df = eeg_df.iloc[eeg_start:eeg_end].copy()\n",
    "        eeg_array = select_eeg_channels(eeg_window_df, list(cfg.eeg.channels))\n",
    "        eeg_graphs = eeg_builder.process_eeg_signal(eeg_array)\n",
    "\n",
    "        spec_path = f\"{cfg.paths.train_spectrograms}/{row.spectrogram_id}.parquet\"\n",
    "        spec_df = pd.read_parquet(spec_path)\n",
    "        spec_offset = int(row.spectrogram_label_offset_seconds)\n",
    "        spec_window_df = spec_df[(spec_df['time'] >= spec_offset) & (spec_df['time'] < spec_offset + 600)].copy()\n",
    "        spec_window_df = filter_spectrogram_columns(spec_window_df, list(cfg.spectrogram.regions))\n",
    "        spec_graphs = spec_builder.process_spectrogram(spec_window_df)\n",
    "\n",
    "        label_to_index = dict(cfg.label_to_index)\n",
    "        target = label_to_index.get(str(row.expert_consensus).strip(), -1)\n",
    "        if target == -1:\n",
    "            continue\n",
    "        patient_data[int(row.label_id)] = {\n",
    "            'eeg_graphs': eeg_graphs,\n",
    "            'spec_graphs': spec_graphs,\n",
    "            'target': int(target),\n",
    "        }\n",
    "\n",
    "    out_path = proc_dir / f'patient_{first_pid}.pt'\n",
    "    if patient_data:\n",
    "        torch.save(patient_data, out_path)\n",
    "        print('Saved minimal patient file:', out_path)\n",
    "    else:\n",
    "        print('Failed to create minimal patient data; please run full preprocessing.')\n",
    "else:\n",
    "    print('Found', len(existing), 'processed patient files; skipping minimal preprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HMS Multi-Modal GNN Training\n",
      "============================================================\n",
      "Config: configs/model.yaml\n",
      "WandB Project: hms-graphs\n",
      "WandB Run: smoke-notebook\n",
      "============================================================\n",
      "\n",
      "[Info] WANDB_MODE=offline (no internet required)\n",
      "Initializing DataModule...\n",
      "\n",
      "============================================================\n",
      "Dataset Setup - Fold 0/4:\n",
      "  Train: 1557 patients, 17358 samples\n",
      "  Val:   393 patients, 2825 samples\n",
      "\n",
      "  Stratification by evaluator bins:\n",
      "    Train: {0: 11795, 1: 241, 2: 3902, 3: 1333, 4: 87}\n",
      "    Val: {0: 1896, 1: 18, 2: 653, 3: 250, 4: 8}\n",
      "\n",
      "  Class weights: None (skipped)\n",
      "============================================================\n",
      "\n",
      "Initializing Model...\n",
      "\n",
      "Model Architecture:\n",
      "  EEG output dim:    256\n",
      "  Spec output dim:   256\n",
      "  Fusion output dim: 512\n",
      "  Num classes:       6\n",
      "  Total parameters:  2,019,846\n",
      "  Trainable params:  2,019,846\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id wxwn4ubz.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>logs/wandb/offline-run-20251102_111043-wxwn4ubz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/graph-ml-2/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | model               | HMSMultiModalGNN   | 2.0 M  | train\n",
      "1 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "2 | train_metrics       | MetricCollection   | 0      | train\n",
      "3 | val_metrics         | MetricCollection   | 0      | train\n",
      "4 | test_metrics        | MetricCollection   | 0      | train\n",
      "5 | train_acc_per_class | MulticlassAccuracy | 0      | train\n",
      "6 | val_acc_per_class   | MulticlassAccuracy | 0      | train\n",
      "7 | test_acc_per_class  | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.079     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset Setup - Fold 0/4:\n",
      "  Train: 1557 patients, 17358 samples\n",
      "  Val:   393 patients, 2825 samples\n",
      "\n",
      "  Stratification by evaluator bins:\n",
      "    Train: {0: 11795, 1: 241, 2: 3902, 3: 1333, 4: 87}\n",
      "    Val: {0: 1896, 1: 18, 2: 653, 3: 250, 4: 8}\n",
      "\n",
      "  Class weights: None (skipped)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1ddd8bc0b94b0d8243341408ed06ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 38. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09b34f56120464280d01760a159dcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 2.122\n",
      "Epoch 0, global step 2: 'val/loss' reached 2.12158 (best 2.12158), saving model to '/workspace/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=2.1216.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing best model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=2.1216.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /workspace/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=2.1216.ckpt\n",
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485b53c84a1b4d51837956582c7031dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 19. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m URL not available in offline run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/acc            0.10300885140895844\n",
      "    test/acc_class_0        0.11460673809051514\n",
      "    test/acc_class_1       0.012987012974917889\n",
      "    test/acc_class_2                0.0\n",
      "    test/acc_class_3        0.2631579041481018\n",
      "    test/acc_class_4        0.6081632375717163\n",
      "    test/acc_class_5       0.0017793594161048532\n",
      "     test/acc_macro         0.08383481204509735\n",
      "      test/f1_macro         0.07551635056734085\n",
      "        test/loss                   nan\n",
      "  test/precision_macro      0.07463131844997406\n",
      "    test/recall_macro       0.08383481204509735\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "Best checkpoint: /workspace/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=2.1216.ckpt\n",
      "WandB run: None\n",
      "============================================================\n",
      "\n",
      "Smoke training done.\n"
     ]
    }
   ],
   "source": [
    "# Smoke training (1 epoch, 2 train batches, 1 val batch)\n",
    "import importlib, src.lightning_trainer.graph_lightning_module as glm\n",
    "importlib.reload(glm)\n",
    "import src.lightning_trainer as lt\n",
    "importlib.reload(lt)\n",
    "import src.train as tr\n",
    "importlib.reload(tr)\n",
    "from src.train import train\n",
    "trainer, model, datamodule = train(\n",
    "    config_path='configs/model.yaml',\n",
    "    wandb_project='hms-graphs',\n",
    "    wandb_name='smoke-notebook',\n",
    "    smoke=True, offline=True,\n",
    "    limit_train_batches=2, limit_val_batches=1, max_epochs_override=1,\n",
    "    batch_size_override=2, num_workers_override=0,\n",
    ")\n",
    "print('Smoke training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: checkpoints/last.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Locate best checkpoint for inference\n",
    "from pathlib import Path\n",
    "ckpt_dir = Path('checkpoints')\n",
    "best_ckpt = None\n",
    "if ckpt_dir.exists():\n",
    "    # Prefer latest by mtime; compatible with our Trainer checkpoint naming\n",
    "    paths = sorted(ckpt_dir.glob('*.ckpt'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    best_ckpt = str(paths[0]) if paths else None\n",
    "print('Best checkpoint:', best_ckpt)\n",
    "assert best_ckpt is not None, 'No checkpoints found after training.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HMS Multi-Modal GNN Training\n",
      "============================================================\n",
      "Config: configs/model.yaml\n",
      "WandB Project: hms-graphs\n",
      "WandB Run: smoke-notebook\n",
      "============================================================\n",
      "\n",
      "[Info] WANDB_MODE=offline (no internet required)\n",
      "Initializing DataModule...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset Setup - Fold 0/4:\n",
      "  Train: 1557 patients, 17358 samples\n",
      "  Val:   393 patients, 2825 samples\n",
      "\n",
      "  Stratification by evaluator bins:\n",
      "    Train: {0: 11795, 1: 241, 2: 3902, 3: 1333, 4: 87}\n",
      "    Val: {0: 1896, 1: 18, 2: 653, 3: 250, 4: 8}\n",
      "\n",
      "  Class weights: None (skipped)\n",
      "============================================================\n",
      "\n",
      "Initializing Model...\n",
      "\n",
      "Model Architecture:\n",
      "  EEG output dim:    256\n",
      "  Spec output dim:   256\n",
      "  Fusion output dim: 512\n",
      "  Num classes:       6\n",
      "  Total parameters:  2,019,846\n",
      "  Trainable params:  2,019,846\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | model               | HMSMultiModalGNN   | 2.0 M  | train\n",
      "1 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "2 | train_metrics       | MetricCollection   | 0      | train\n",
      "3 | val_metrics         | MetricCollection   | 0      | train\n",
      "4 | test_metrics        | MetricCollection   | 0      | train\n",
      "5 | train_acc_per_class | MulticlassAccuracy | 0      | train\n",
      "6 | val_acc_per_class   | MulticlassAccuracy | 0      | train\n",
      "7 | test_acc_per_class  | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.079     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | model               | HMSMultiModalGNN   | 2.0 M  | train\n",
      "1 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "2 | train_metrics       | MetricCollection   | 0      | train\n",
      "3 | val_metrics         | MetricCollection   | 0      | train\n",
      "4 | test_metrics        | MetricCollection   | 0      | train\n",
      "5 | train_acc_per_class | MulticlassAccuracy | 0      | train\n",
      "6 | val_acc_per_class   | MulticlassAccuracy | 0      | train\n",
      "7 | test_acc_per_class  | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.079     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset Setup - Fold 0/4:\n",
      "  Train: 1557 patients, 17358 samples\n",
      "  Val:   393 patients, 2825 samples\n",
      "\n",
      "  Stratification by evaluator bins:\n",
      "    Train: {0: 11795, 1: 241, 2: 3902, 3: 1333, 4: 87}\n",
      "    Val: {0: 1896, 1: 18, 2: 653, 3: 250, 4: 8}\n",
      "\n",
      "  Class weights: None (skipped)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbaf9902fd546e981b38207c625db20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 38. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7847c60cdbe467480b7c2e1166b9026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 1.917\n",
      "Epoch 0, global step 2: 'val/loss' reached 1.91650 (best 1.91650), saving model to '/root/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=1.9165.ckpt' as top 3\n",
      "Epoch 0, global step 2: 'val/loss' reached 1.91650 (best 1.91650), saving model to '/root/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=1.9165.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing best model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=1.9165.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /root/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=1.9165.ckpt\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n",
      "Loaded model weights from the checkpoint at /root/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=1.9165.ckpt\n",
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbebbda94b9347c0985da488230dcf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/mamba/envs/graph-ml-2/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 19. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m URL not available in offline run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/acc            0.35929203033447266\n",
      "    test/acc_class_0        0.10898876190185547\n",
      "    test/acc_class_1                0.0\n",
      "    test/acc_class_2        0.06930693238973618\n",
      "    test/acc_class_3                0.0\n",
      "    test/acc_class_4                0.0\n",
      "    test/acc_class_5        0.8042704463005066\n",
      "     test/acc_macro         0.33421841263771057\n",
      "      test/f1_macro         0.3019481897354126\n",
      "        test/loss                   nan\n",
      "  test/precision_macro      0.2935105264186859\n",
      "    test/recall_macro       0.33421841263771057\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "Best checkpoint: /root/Kaggle-HMS-2/checkpoints/hms-epoch=00-val/loss=1.9165.ckpt\n",
      "WandB run: None\n",
      "============================================================\n",
      "\n",
      "Smoke training done.\n"
     ]
    }
   ],
   "source": [
    "# Inference on test split (from processed training data) with timing logs\n",
    "import importlib, src.predict as sp\n",
    "importlib.reload(sp)\n",
    "from src.predict import predict\n",
    "from pathlib import Path\n",
    "\n",
    "pred_out = 'notebooks/outputs/preds_smoke.csv'\n",
    "Path('notebooks/outputs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use the best (latest) checkpoint discovered earlier\n",
    "print('Using checkpoint:', best_ckpt)\n",
    "\n",
    "predict(\n",
    "    config_path='configs/model.yaml',\n",
    "    checkpoint_path=best_ckpt,\n",
    "    output_csv=pred_out,\n",
    "    batch_size_override=2,\n",
    "    num_workers_override=0,\n",
    "    max_batches=2,  # limit for quick verification; remove for full inference\n",
    ")\n",
    "print('Saved predictions to', pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /root/Kaggle-HMS-2\n",
      "WANDB_MODE = offline\n",
      "Processed patients: 1950\n"
     ]
    }
   ],
   "source": [
    "# Setup & Imports\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "print('Project root:', project_root)\n",
    "\n",
    "# WANDB offline for smoke runs\n",
    "os.environ.setdefault('WANDB_MODE', 'offline')\n",
    "print('WANDB_MODE =', os.environ.get('WANDB_MODE'))\n",
    "\n",
    "# Quick data check\n",
    "from pathlib import Path\n",
    "proc_dir = Path('data/processed')\n",
    "assert proc_dir.exists(), f'Missing processed data at {proc_dir}. Run preprocessing first.'\n",
    "patient_files = list(proc_dir.glob('patient_*.pt'))\n",
    "print('Processed patients:', len(patient_files))\n",
    "assert len(patient_files) > 0, 'No processed patient files found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMS Graphs: Smoke Train + Inference\n",
    "#\n",
    "# This notebook runs a fast smoke test training and a quick inference pass\n",
    "# using the processed graph data in `data/processed/`.\n",
    "# - Training: 1 epoch, 2 train batches, 1 val batch, WANDB offline\n",
    "# - Inference: runs on the test split from processed training data\n",
    "#\n",
    "# After verifying this works, run full training from terminal (see notes below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "isSourceIdPinned": false,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 8611582,
     "sourceId": 13557778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8612852,
     "sourceId": 13559487,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8612404,
     "sourceId": 13578948,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8626737,
     "sourceId": 13578958,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
